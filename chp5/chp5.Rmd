---
title: "chp5"
author: "Vasco Brazão"
date: "15/02/2021"
output: pdf_document
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
library(tidyverse)
library(here)
library(brms)
```
## 5E1

(4) would be the standard way to write a multiple linear regression. I Suppose (2) could be valid? If we force the intercept to be 0. (3) seems plausible, but from the lack of an index on beta I would think you're forcing the beta for x to be equal to -1 * the beta for z, which is.. strange?

Van Bussel agrees https://github.com/castels/StatisticalRethinking/blob/master/Chapter%205/VanBussel_Chapter5_Questions.pdf



## 5E2

mu_latitude_i = alpha + beta_adiv * adiv_i + beta_pdiv * pdiv_i

## 5E3

$$time_i \sim \text{Normal}(\mu, \sigma)$$
$$\mu_i = \alpha + \beta_f f_i + \beta_s s_i $$
Both slope parameters should be positive. 

Van Bussel agrees!

But I still can't make a stupid latex document. One day. 

## 5E4

1, 3, 4 would be my guesses.

Van Bussel disagrees - 4 is not correct. But I still think it works?

And a latex document thing was created! I cannot believe my eyes. What fresh hell awaits me now? We shall see.. we shall see.

## 5M1

Inventing a spurious correlation.

Let's say that both British and German tourists like going to sunny places. For a given year, they base their decision of whether to go Portugal for vacation on the number of sunlight days of the previous year. Germans respond more strongly to this parameter because they now have an easier time going to Portugal than Brits, and they also stray less from the line because they are German.

```{r spurious}
n <- 1000

df <- tibble(
  sun = rnorm(n, 0, 1),
  brits = 0.5*sun + rnorm(n, 0, 1),
  germs = 0.9*sun + rnorm(n, 0, 0.2)
)
```

```{r}
pairs(df)
```


There appears to be a positive relationship betweet amount of brits and amount of germans.

```{r 5m1}
m1 <- lm(df$brits ~ df$germs)
m2 <- lm(df$brits ~ df$germs + df$sun)

summary(m1)
summary(m2)
```
Et voila. when sun is in the model, knowing how many germans went to Portugal does not really tell us much more about how many brits went.

## 5M2

Now imagine a third country. Those citizens go to Portugal in higher numbers when they know their German friends will be going, but they prefer to avoid Brits on the beaches, for whatever reason.

```{r 5m2}
df <- df %>%
  mutate(
    third = rnorm(n, mean = germs - brits)
  )

pairs(df)
```
```{r 5m2.models}
lm(third ~ germs, data = df)
lm(third ~ brits, data = df)
lm(third ~ germs + brits, data = df)
```
Indeed! When both are included in the model, the coefficients are much greater. When only one is included, its relationship is masked. 

## 5M3

If, in one year, a person marries, divorces, and marries again, they will have added two counts of marriage for that one year -- marriage rate is affected by remarriages. Thus, in a society where divorce is accepted but being married is highly desirable, we could reasonably assume a casual link wherein the divorce rate influences the marriage rate. In years with (say) 0 divorces, marriages would mostly be first marriages. In years with a very high number of divorces, we might see that same baseline of new marriages PLUS a surge of remarriages driving up the marriage rate.

You could do a simple linear regression, though that would not inform you about the direction of causality. Alternatively, if you have reason to believe that the time between divorce and remarriage is about a year, you should see an effect of divorce at year 0 on marriage at year 1, but not so much the other way around.

## 5M4

Found the LDS data on wikipedia. 

```{r}
data(WaffleDivorce, package = "rethinking")

d <- WaffleDivorce %>% as_tibble()
lds <- readxl::read_xlsx(path = here("chp5/ldsdata.xlsx"),
                         col_names = TRUE)

d2 <- full_join(d, lds, by = c("Location" = "State")) %>%
  select(Loc, MedianAgeMarriage, Marriage, Divorce, LDS) %>%
  filter(!is.na(Loc)) %>%
  mutate(
    across(.cols = MedianAgeMarriage:LDS,
           .fns = ~ rethinking::standardize(.))
  )
```

Now it's all standardized and in `d2`.

```{r 5m4}
lm(Divorce ~ MedianAgeMarriage + Marriage + LDS, data = d2) %>% summary


b5m4 <- 
   brm(data = d2, 
       family = gaussian,
       Divorce ~ 1 + MedianAgeMarriage + Marriage + LDS,
       prior = c(prior(normal(0, 0.2), class = Intercept),
                 prior(normal(0, 0.5), class = b),
                 prior(exponential(1), class = sigma)),
       iter = 2000, warmup = 1000, chains = 4, cores = 4,
       seed = 5,
       sample_prior = T,
       file = "b5m4")

print(b5m4)

summary(b5m4)
```

## 5M5

A DAG might be useful here. 

```{r 5m5}
ggdag::dagify(
  Obesity ~ Exercise + EatingOut,
  Exercise ~ Driving,
  EatingOut ~ Driving, 
  Driving ~ GasPrice
) %>%
  ggdag::ggdag(node_size = 22) +
  theme_void()


```
How ugly! And it changes every time I run it. I embrace the chaos.

Ok, so...

* We could regress obesity on each of the predictors separately, just to confirm our intuitions.
* We could regress obesity on gas price and driving. We would expect that the coefficent for GasPrice would shrink, since Driving would be holding most of the information
* We could regress obesity on EatingOut and Exercise. The coefficients should shrink because they are collinear. 
* We could regress obesity on EatingOut, Exercise, and Driving. The coefficient for Driving should shrink?

## 5H1

```{r 5h1}
dag <- dagitty::dagitty(
  "dag{M -> A; A -> D}",
  layout = TRUE
)

plot(dag)

dagitty::impliedConditionalIndependencies(dag)
```

Without conditioning, all pairs of variables should be associated. 

D is independent of M, conditioned on A. (A here is the mediator)

This is consistent with the pattern of results from the three models (m5.1, m5.2, and m5.3).

## 5H2

Taking the previous DAG, I am to fit a model and use it to estimate the effect of halving a State's marriage rate (M). 

Side-note: I recentely read this blogpost: https://elevanth.org/blog/2018/07/14/statistical-rethinking-edition-2-eta-2020/

In which Richard says: 

> "First, I force the reader to explicitly specify every assumption of the model. Some readers of the first edition lobbied me to use simplified formula tools like brms or rstanarm. Those are fantastic packages, and graduating to use them after this book is recommended. But I don’t see how a person can come to understand the model when using those tools. The priors being hidden isn’t the most limiting part. Instead, since linear model formulas like y ~ (1|x) + z don’t show the parameters, nor even all of the terms, it is not easy to see how the mathematical model relates to the code. It is ultimately kinder to be a bit cruel and require more work. So the formula lists remain. In this book, you are programming the log-posterior, down to the exact relationship between each variable and coefficient. You’ll thank me later."

So I just decided that I will use the rethinking package to fit the models after all, and then do them in brms as well. This is becoming a giant mess. I embrace the chaos.

```{r 5h2.data}
# we will want these for later: mean and sd od MedianAgeMarriage
M_mean <- mean(d$Marriage)
M_sd <- sd(d$Marriage)

d2 <- d2 %>% 
  mutate(
    M = Marriage,
    A = MedianAgeMarriage,
    D = Divorce
  )
```

Now the  models we need to estimate would be M -> A, and then A -> D, right? No need for a model that has M and A?

wjakethompson disagrees (see https://github.com/wjakethompson/sr2-solutions/blob/main/03-more-linear-models.Rmd). I will include both then, but I'm not very confident. 


```{r 5h2.quap}
m5h2 <- rethinking::quap(
  alist(
    ## M -> A -> D
    D ~ dnorm(mu, sigma),
    mu <- a + bA*A + bM*M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1),
    
    ## M -> A
    A ~ dnorm(mu_A, sigma_A),
    mu_A <- aA + bAA*M,
    aA ~ dnorm(0, 0.2),
    bAA ~ dnorm(0, 0.5),
    sigma_A ~ dexp(1)
  ), data = d2
)

rethinking::precis(m5h2)


M_seq <- seq(from = -3, to = 3, length.out = 50)

# now to simulate and plot the counterfactual

sim_dat <- data.frame(M = M_seq)

s <- rethinking::sim(m5h2,
                     data = sim_dat,
                     vars = c("A", "D"))

plot(sim_dat$M * M_sd + M_mean, colMeans(s$D), ylim = c(-2, 2), type = "l")
rethinking::shade(apply(s$D, 2, rethinking::PI), sim_dat$M * M_sd + M_mean)
```

The effect of halving the marriage rate depends on the rate itself. From M = 30 to M = 15, Divorce falls about 1SD; from M = 20 to M = 10, the fall is  a bit smaller than 1SD.

But we can do a bit better: if

$$D = \alpha + \beta M $$

Then 

$$D_{\frac{1}{2}M} = \alpha + \beta * (\frac{1}{2} M)  $$

And we can use these to say exactly how we predict D to change when we halve M. The *difference* between the new value and the original value will be

$$
D_{\frac{1}{2}M} - D = \alpha + \beta * (\frac{1}{2} M) - (\alpha + \beta M)
$$

Which we simplify to 
$$
D_{\frac{1}{2}M} - D = -\frac{1}{2}\beta M
$$
```{r}
unscaled <- NULL
unscaled$M <- sim_dat$M * M_sd + M_mean


lm(colMeans(s$D) ~ (unscaled$M) )
```

From this simple linear model we estimate $\beta = 0.095$. As a sanity check, we calculate that the difference in Divorce rates when we go from M = 30 to M = 15 should equal 

```{r}
-0.5 * 0.095 * 30
```
This is a bigger jump than I predicted. Maybe it's hard to see from the graph?

Next, doing it in `brms` and the `tidyverse`. 