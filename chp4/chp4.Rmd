---
title: "chp4"
author: "Vasco Braz√£o"
date: "12/23/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages & Data
```{r packages}
library(tidyverse)
library(here)
library(tinytex)
library(rethinking)

data(Howell1)

d <- Howell1

rm(Howell1)
detach(package:rethinking, unload = T)
library(brms)
```

## 4E1

The first line is the likelihood, the two others lines are the priors for mu and sigma.

## 4E2

2 parameters, mu and sigma

## 4E3

Oh god. Still need to learn how to format things so...

Pr(mu, sigma|y_i) = PRODUCT_i(Normal(y_i|mu, sigma) * Normal(mu|0, 10) * Exponential(sigma|1) / DOUBLE_INTEGRAL(PRODUCT_i(Normal(y_i|mu, sigma) * Normal(mu|0, 10) * Exponential(sigma|1) )) dmu dsigma

Happy?

## 4E4

The second line is the linear model.

## 4E5

There are three parameters: alpha, beta, and sigma.

## 4M1

```{r 4m1.sim}
N <- 100000

y_values <- tibble(
  mu = rnorm(N, 0, 10),
  sigma = rexp(N, 1),
  y = rnorm(N, mean = mu, sd = sigma)
)


hist(y_values$y, breaks = 1000)
```

## 4M2

As I'm using `brms`, and trying to learn how to use it at the same time, I'll skip the `quap` stuff and jump straight into `brms`.

```{r 4m2}
#this is what it maybe would look like in brms. hell if I know

b4m1 <- brm(data = y_values,
            family = gaussian,
            y ~ 1,
            prior = c(prior(normal(0, 10), class = Intercept),
                      prior(exponential(1), class = sigma)),
            iter = 2000, warmup = 1000, chains = 4, cores = 4,
            seed = 4,
            file = here("b4m1")
)
```

```{r 4m2.chains}

plot(b4m1)

```

## 4M4

$$
\begin{align*}
\text{height}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \alpha + \beta \times (\text{year}_i - 2) \\
\alpha & \sim \operatorname{Normal}(160, 10) \\
\beta  & \sim \operatorname{Normal}(0, 5) \\
\sigma & \sim \operatorname{Exponential}(1/10) \\
\end{align*}
$$

## 4M5

Of course, I almost forgot!

$$
\begin{align*}
\text{height}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \alpha + \beta \times (\text{year}_i - 2) \\
\alpha & \sim \operatorname{Normal}(140, 20) \\
\beta  & \sim \operatorname{Log-Normal}(0, 5) \\
\sigma & \sim \operatorname{Exponential}(1/10) \\
\end{align*}
$$

## 4M6 

Assuming also that our sample has kids of roughly the same age, we can adjust the prior for the variance. But not sure this one makes sense?

$$\text{height}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \alpha + \beta \times (\text{year}_i - 2) \\
\alpha & \sim \operatorname{Normal}(140, 32) \\
\beta  & \sim \operatorname{Log-Normal}(0, 5) \\
\sigma & \sim \operatorname{Exponential}(1/32)$$

## 4M7

```{r 4m7}
d2 <- d %>%
  filter(age >= 18)

b4.3 <- 
  brm(data = d2, 
      family = gaussian,
      height ~ 1 + weight,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma)),
      iter = 28000, warmup = 27000, chains = 4, cores = 4,
      seed = 4,
      file = "b04.03")
```
So brms has some warnings and I'm not sure I need to care about them?

```{r}
plot(b4.3)
```
```{r}
posterior_summary(b4.3)[1:3, ] %>% 
  round(digits = 2)
```

So, to compare with the fit where weight was centered:

* the estimate for b_intercept is lower for my model (by around 41kg), and the error is almost 7 times higher.
* the estimate for beta is almost exactly the same
* the estimate for sigma is almost exactly the same

```{r}
vcov(b4.3) %>%
  round(3)
```
Now, though, the variance of the intercept is way larger than before, and the covariance with weight is slightly negative (vs 0)

To get sigma as well:

```{r}
posterior_samples(b4.3) %>%
  select(-lp__) %>%
  cov() %>%
  round(digits = 3)
```
Posterior against the data:
```{r}
d2 %>%
  ggplot(aes(x = weight, y = height)) +
  geom_abline(intercept = fixef(b4.3)[1], 
              slope     = fixef(b4.3)[2]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  theme_classic()
```

Making posterior predictions:

```{r}
weight_seq <- 
  tibble(weight = 25:70)

mu_summary <-
  fitted(b4.3, 
         newdata = weight_seq) %>%
  data.frame() %>%
  bind_cols(weight_seq)

pred_height <-
  predict(b4.3,
          newdata = weight_seq) %>%
  data.frame() %>%
  bind_cols(weight_seq)

d2 %>%
  ggplot(aes(x = weight)) +
  geom_ribbon(data = pred_height, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2$weight),
                  ylim = range(d2$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```
They look exactly the same?