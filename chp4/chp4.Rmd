---
title: "chp4"
author: "Vasco Braz√£o"
date: "12/23/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages & Data
```{r packages}
library(tidyverse)
library(here)
library(tinytex)
library(rethinking)

data(Howell1)

d <- Howell1

rm(Howell1)
detach(package:rethinking, unload = T)
library(brms)
```

## 4E1

The first line is the likelihood, the two others lines are the priors for mu and sigma.

## 4E2

2 parameters, mu and sigma

## 4E3

Oh god. Still need to learn how to format things so...

Pr(mu, sigma|y_i) = PRODUCT_i(Normal(y_i|mu, sigma) * Normal(mu|0, 10) * Exponential(sigma|1) / DOUBLE_INTEGRAL(PRODUCT_i(Normal(y_i|mu, sigma) * Normal(mu|0, 10) * Exponential(sigma|1) )) dmu dsigma

Happy?

## 4E4

The second line is the linear model.

## 4E5

There are three parameters: alpha, beta, and sigma.

## 4M1

```{r 4m1.sim}
N <- 100000

y_values <- tibble(
  mu = rnorm(N, 0, 10),
  sigma = rexp(N, 1),
  y = rnorm(N, mean = mu, sd = sigma)
)


hist(y_values$y, breaks = 1000)
```

## 4M2

As I'm using `brms`, and trying to learn how to use it at the same time, I'll skip the `quap` stuff and jump straight into `brms`.

```{r 4m2}
#this is what it maybe would look like in brms. hell if I know

b4m1 <- brm(data = y_values,
            family = gaussian,
            y ~ 1,
            prior = c(prior(normal(0, 10), class = Intercept),
                      prior(exponential(1), class = sigma)),
            iter = 2000, warmup = 1000, chains = 4, cores = 4,
            seed = 4,
            file = here("b4m1")
)
```

```{r 4m2.chains}

plot(b4m1)

```

## 4M4

$$
\begin{align*}
\text{height}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \alpha + \beta \times (\text{year}_i - 2) \\
\alpha & \sim \operatorname{Normal}(160, 10) \\
\beta  & \sim \operatorname{Normal}(0, 5) \\
\sigma & \sim \operatorname{Exponential}(1/10) \\
\end{align*}
$$

## 4M5

Of course, I almost forgot!

$$
\begin{align*}
\text{height}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \alpha + \beta \times (\text{year}_i - 2) \\
\alpha & \sim \operatorname{Normal}(140, 20) \\
\beta  & \sim \operatorname{Log-Normal}(0, 5) \\
\sigma & \sim \operatorname{Exponential}(1/10) \\
\end{align*}
$$

## 4M6 

Assuming also that our sample has kids of roughly the same age, we can adjust the prior for the variance. But not sure this one makes sense?

$$
\begin{align*}
\text{height}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \alpha + \beta \times (\text{year}_i - 2) \\
\alpha & \sim \operatorname{Normal}(140, 32) \\
\beta  & \sim \operatorname{Log-Normal}(0, 5) \\
\sigma & \sim \operatorname{Exponential}(1/32)
\end{align*}
$$

## 4M7

```{r 4m7}
d2 <- d %>%
  filter(age >= 18)

b4.3 <- 
  brm(data = d2, 
      family = gaussian,
      height ~ 1 + weight,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma)),
      iter = 28000, warmup = 27000, chains = 4, cores = 4,
      seed = 4,
      file = "b04.03")
```
So brms has some warnings and I'm not sure I need to care about them?

```{r}
plot(b4.3)
```
```{r}
posterior_summary(b4.3)[1:3, ] %>% 
  round(digits = 2)
```

So, to compare with the fit where weight was centered:

* the estimate for b_intercept is lower for my model (by around 41kg), and the error is almost 7 times higher.
* the estimate for beta is almost exactly the same
* the estimate for sigma is almost exactly the same

```{r}
vcov(b4.3) %>%
  round(3)
```
Now, though, the variance of the intercept is way larger than before, and the covariance with weight is slightly negative (vs 0)

To get sigma as well:

```{r}
posterior_samples(b4.3) %>%
  select(-lp__) %>%
  cov() %>%
  round(digits = 3)
```
Posterior against the data:
```{r}
d2 %>%
  ggplot(aes(x = weight, y = height)) +
  geom_abline(intercept = fixef(b4.3)[1], 
              slope     = fixef(b4.3)[2]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  theme_classic()
```

Making posterior predictions:

```{r}
weight_seq <- 
  tibble(weight = 25:70)

mu_summary <-
  fitted(b4.3, 
         newdata = weight_seq) %>%
  data.frame() %>%
  bind_cols(weight_seq)

pred_height <-
  predict(b4.3,
          newdata = weight_seq) %>%
  data.frame() %>%
  bind_cols(weight_seq)

d2 %>%
  ggplot(aes(x = weight)) +
  geom_ribbon(data = pred_height, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2$weight),
                  ylim = range(d2$height)) +
  theme(panel.grid = element_blank())
```
They look exactly the same?

## 4M8
First, I want to fit the same model as in the chapter. Need to get the data first.

(Upon second thought: I will skip the exercises with splines for now. I want to understand brms better before I come back to this. So for now I'll do only H1 to H4.)

```{r}
#detach(package:brms, unload = T)
#library(rethinking)
#
#data(cherry_blossoms)
#d <- cherry_blossoms
#rm(cherry_blossoms)
#
#d2 <- d %>%
#  tidyr::drop_na(doy)
```

## 4H1
First, let's just make a dataset with our weights

```{r 4h1.weights}
new_kung <- tibble(
  id = 1:5,
  weight = c(46.95, 43.72, 64.78, 32.59, 54.63)
)
```

I'm working with the same priors as the brms implementation, section 4.4.2. I hope to become confident enough to set my own priors soon...

$$
\begin{align*}
\text{height}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \alpha + \beta \times (\text{weight}_i - \bar{weight}) \\
\alpha & \sim \operatorname{Normal}(178, 20) \\
\beta  & \sim \operatorname{Log-Normal}(0, 1) \\
\sigma & \sim \operatorname{Uniform}(0, 50) \\
\end{align*}
$$

```{r 4H1}
d2 <-
  d2 %>% 
  mutate(weight_c = weight - mean(weight))

# formula from brms implementation, section 4.4.2
b4.3 <- 
  brm(data = d2, 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma)),
      iter = 28000, warmup = 27000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03")
```

```{r 4h1.contd}
new_kung <- new_kung %>%
  mutate(
    weight_c = weight - mean(d2$weight)
  )

mu <-
  fitted(b4.3,
         summary = F,
         newdata = new_kung) %>%
  data.frame() %>%
  set_names(new_kung$weight) %>%
  mutate(iter = 1:n()) %>%
  pivot_longer(-iter,
               names_to = "weight",
               values_to = "height") %>%
  mutate(weight = as.numeric(weight))

pred_height <-
  predict(b4.3,
          newdata = new_kung,
          probs = c(.055, .945)) %>%
  data.frame() %>%
  bind_cols(new_kung)

pred_height
```

`pred_height` has what we need! Estimate = expected height; and the bounds of the 89% interval are in the Q5.5. and Q94.5 columns.

## 4H2

```{r 4h2.data}
d3 <- d %>%
  filter(age < 18)
```

`d3` has 192 observations, so far so good.

```{r 4h2.fit}
d3 <- d3 %>%
  mutate(
    weight_c = weight - mean(d3$weight)
  )


b4h2 <- 
  brm(data = d3, 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(prior(normal(100, 50), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma)),
      iter = 28000, warmup = 27000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b4h2")
```

```{r 4h2.summary}
summary(b4h2)
```
(a) From what I understand, this means the model expects a 2.72cm increase in height for every extra kg of weight.

```{r 4h2.plot}
d3 %>%
  ggplot(aes(x = weight_c, y = height)) +
  geom_abline(intercept = fixef(b4h2)[1], 
              slope     = fixef(b4h2)[2]) +
  geom_point()
```

```{r 4h2.b}
weight_c_seq <- 
  tibble(weight_c = -15:25)

mu_summary <-
  fitted(b4h2, 
         newdata = weight_c_seq,
         probs = c(0.055, 0.945)) %>%
  data.frame() %>%
  bind_cols(weight_c_seq)

pred_height <-
  predict(b4h2,
          newdata = weight_c_seq,
          probs = c(0.055, 0.945)) %>%
  data.frame() %>%
  bind_cols(weight_c_seq)

d3 %>%
  ggplot(aes(x = weight_c))  + 
  geom_ribbon(data = pred_height, 
              aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),
              fill = "grey83") +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = height))
```
(c) It seems the model is poor over all, but poorest around the edges: it overestimates height both for low and for high weight.

## 4H3

Ok, so we shall now model height as dependent upon the log of weight. 

A bit like...

$$
\begin{align*}
\text{height}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \alpha + \beta \times \operatorname{Log}(\text{weight}_i) \\
\alpha & \sim \operatorname{Normal}(178, 20) \\
\beta  & \sim \operatorname{Log-Normal}(0, 1) \\
\sigma & \sim \operatorname{Uniform}(0, 50) \\
\end{align*}
$$

?

And how does that look in brms?

```{r 4h3.fit}
b4h3 <- 
  brm(data = d, 
      family = gaussian,
      height ~ 1 + log(weight),
      prior = c(prior(normal(100, 50), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma)),
      iter = 28000, warmup = 27000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b4h3.correct")
```

```{r 4h3.model}
plot(b4h3)

summary(b4h3)
```
Not sure I can interpret the parameters tbh.

```{r 4h3.plots}
d %>%
  ggplot(aes(x = weight)) +
  geom_point(aes(y = height))

new_weight <- tibble(
  weight = 5:65
)

mu_summary <-
  fitted(b4h3, 
         newdata = new_weight,
         probs = c(0.015, 0.985)) %>%
  data.frame() %>%
  bind_cols(new_weight)

pred_height <-
  predict(b4h3,
          newdata = new_weight,
          probs = c(0.015, 0.985)) %>%
  data.frame() %>%
  bind_cols(new_weight)


d %>% ggplot(aes(x = weight)) +
  geom_ribbon(data = pred_height, 
              aes(y = Estimate, ymin = Q1.5, ymax = Q98.5),
              fill = "grey83") +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q1.5, ymax = Q98.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, size = 1/2) +
  geom_point(data = d, aes(y = height), alpha = .7) +
  theme_bw()
```

## 4H4

The model in chapter was:

h_i = Normal(mu_i, sigma)

mu_i = alpha + b_1 * x_i, b_2 * x_i^2

alpha = normal(178,20)
b_1 = Log-Normal(0,1)
b_2 = Normal(0,1)
sigma = uniform(0,50)

x_i is the weight of the individual.

```{r 4h4}
n_lines <- 100

lines <- 
  tibble(n = 1:n_lines,
         a = rnorm(n_lines, mean = 178, sd = 20),
         b1 = rlnorm(n_lines, meanlog = 0, sdlog = 1),
         b2 = rnorm(n_lines, mean = 0, sd = 1)) %>%
  expand(nesting(n, a, b1, b2), weight = range(d$weight)) %>%
  mutate(height = a + b1 * weight + b2 * weight ^ 2)

lines %>%
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_line(alpha = 1/10) +
  theme_classic()
```
Ok.

I feel like the lines should curve? Either way, time to try to make the model more plausible. E.g.... there cannot be negative mean heights, and they probably shouldn't go as far up as many thousand cm ...

```{r 4h4.adjust}
n_lines <- 100

lines <- 
  tibble(n = 1:n_lines,
         a = rnorm(n_lines, mean = 100, sd = 30),
         b1 = rlnorm(n_lines, meanlog = -5, sdlog = 1),
         b2 = rexp(n_lines, rate = 100)) %>%
  expand(nesting(n, a, b1, b2), weight = range(d$weight)) %>%
  mutate(height = a + b1 * weight + b2 * weight ^ 2)

lines %>%
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, size = 1/3) +
  geom_line(alpha = 1/10) +
  theme_classic()
```
Ok, I am sort of happy with this. 

But I have lingering doubts... shouldn't the lines be curves?